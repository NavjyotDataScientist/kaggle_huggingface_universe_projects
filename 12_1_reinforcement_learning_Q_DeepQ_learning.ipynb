{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNd5GyZkBAQ/Mbl0essHMme",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NavjyotDataScientist/kaggle_huggingface_universe_projects/blob/main/12_1_reinforcement_learning_Q_DeepQ_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep Q-Networks (DQN)\n",
        "What changes?\n",
        "\n",
        "Instead of:\n",
        "\n",
        "Q-table\n",
        "\n",
        "\n",
        "We use:\n",
        "\n",
        "Neural Network â‰ˆ Q-function\n",
        "\n",
        "Why?\n",
        "\n",
        "Because:\n",
        "\n",
        "Q-table explodes\n",
        "\n",
        "States are too many (images, pixels, sensors)"
      ],
      "metadata": {
        "id": "2RT8FsTju0bj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Deep Q-Learning:\n",
        "\n",
        "# Student learns patterns, not memorization.\n",
        "\n",
        "# Understands concepts\n",
        "\n",
        "# Can answer new unseen questions\n",
        "\n",
        "# ðŸ‘‰ Neural network = pattern learner"
      ],
      "metadata": {
        "id": "8Fme4h2Iu75O"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Deep Q-Learning: Teach a child to ride a bicycle (speed 0 â†’ 5)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "\n",
        "# -------------------------------\n",
        "# Child's Brain (Neural Network)\n",
        "# -------------------------------\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(1, 16),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(16, 2)   # actions: 0=pedal, 1=slow\n",
        ")\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_fn = nn.MSELoss()\n",
        "gamma = 0.9  # future reward importance\n",
        "\n",
        "# -------------------------------\n",
        "# Environment (Bicycle rules)\n",
        "# -------------------------------\n",
        "def environment(speed, action):\n",
        "    if action == 0:\n",
        "        speed += 1\n",
        "    else:\n",
        "        speed -= 1\n",
        "\n",
        "    speed = max(0, speed)\n",
        "\n",
        "    if speed == 5:\n",
        "        return speed, 10, True   # success\n",
        "    else:\n",
        "        return speed, -1, False  # fall / imbalance\n",
        "\n",
        "# -------------------------------\n",
        "# Training (Practice)\n",
        "# -------------------------------\n",
        "for episode in range(30):\n",
        "    speed = 0  # start\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        state = torch.tensor([[speed]], dtype=torch.float32)\n",
        "\n",
        "        # Child thinks\n",
        "        q_values = model(state)\n",
        "\n",
        "        # Exploration vs exploitation\n",
        "        if random.random() < 0.2:\n",
        "            action = random.randint(0, 1)\n",
        "        else:\n",
        "            action = torch.argmax(q_values).item()\n",
        "\n",
        "        # Child acts and falls / balances\n",
        "        next_speed, reward, done = environment(speed, action)\n",
        "        next_state = torch.tensor([[next_speed]], dtype=torch.float32)\n",
        "\n",
        "        # Teacher correction (Q-learning rule)\n",
        "        with torch.no_grad():\n",
        "            target = reward + gamma * torch.max(model(next_state))\n",
        "\n",
        "        predicted = q_values[0, action]\n",
        "        loss = loss_fn(predicted, target)\n",
        "\n",
        "        # Learning happens HERE\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        speed = next_speed\n",
        "\n",
        "    print(f\"Episode {episode+1}: Reached speed {speed}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2Y8G98a1uXn",
        "outputId": "32aa7a5a-482e-4ec4-86c7-65ec1decbfe6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1: Reached speed 5\n",
            "Episode 2: Reached speed 5\n",
            "Episode 3: Reached speed 5\n",
            "Episode 4: Reached speed 5\n",
            "Episode 5: Reached speed 5\n",
            "Episode 6: Reached speed 5\n",
            "Episode 7: Reached speed 5\n",
            "Episode 8: Reached speed 5\n",
            "Episode 9: Reached speed 5\n",
            "Episode 10: Reached speed 5\n",
            "Episode 11: Reached speed 5\n",
            "Episode 12: Reached speed 5\n",
            "Episode 13: Reached speed 5\n",
            "Episode 14: Reached speed 5\n",
            "Episode 15: Reached speed 5\n",
            "Episode 16: Reached speed 5\n",
            "Episode 17: Reached speed 5\n",
            "Episode 18: Reached speed 5\n",
            "Episode 19: Reached speed 5\n",
            "Episode 20: Reached speed 5\n",
            "Episode 21: Reached speed 5\n",
            "Episode 22: Reached speed 5\n",
            "Episode 23: Reached speed 5\n",
            "Episode 24: Reached speed 5\n",
            "Episode 25: Reached speed 5\n",
            "Episode 26: Reached speed 5\n",
            "Episode 27: Reached speed 5\n",
            "Episode 28: Reached speed 5\n",
            "Episode 29: Reached speed 5\n",
            "Episode 30: Reached speed 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WocyABq212pR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}