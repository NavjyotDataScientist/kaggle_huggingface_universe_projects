{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSJpSaxLnqrJNin8GbuOme",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NavjyotDataScientist/kaggle_huggingface_universe_projects/blob/main/12_2_reinforcement_policygradient_baseline_critic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WHY POLICY GRADIENT EXISTS\n",
        "\n",
        "Q-learning problems:\n",
        "\n",
        "Hard with continuous actions\n",
        "\n",
        "Max operation is unstable\n",
        "\n",
        "Overestimation bias\n",
        "\n",
        "Policy Gradient:\n",
        "✅ Works with continuous actions\n",
        "✅ Learns smooth behavior\n",
        "❌ More variance (noisy)"
      ],
      "metadata": {
        "id": "fLKxWpbQQxeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Policy Gradient: Teach a child riding style (0 → 5)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "\n",
        "# -----------------------------\n",
        "# Policy Network (Brain)\n",
        "# -----------------------------\n",
        "policy = nn.Sequential(\n",
        "    nn.Linear(1, 16),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(16, 2),\n",
        "    nn.Softmax(dim=1)  # probabilities\n",
        ")\n",
        "\n",
        "optimizer = optim.Adam(policy.parameters(), lr=0.01)\n",
        "gamma = 0.9\n",
        "\n",
        "# -----------------------------\n",
        "# Environment\n",
        "# -----------------------------\n",
        "def environment(speed, action):\n",
        "    speed += 1 if action == 0 else -1\n",
        "    speed = max(0, speed)\n",
        "\n",
        "    if speed == 5:\n",
        "        return speed, 10, True\n",
        "    return speed, -1, False\n",
        "\n",
        "# -----------------------------\n",
        "# Training\n",
        "# -----------------------------\n",
        "for episode in range(30):\n",
        "    speed = 0\n",
        "    log_probs = []\n",
        "    rewards = []\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        state = torch.tensor([[speed]], dtype=torch.float32)\n",
        "\n",
        "        probs = policy(state)\n",
        "        dist = torch.distributions.Categorical(probs)\n",
        "        action = dist.sample()\n",
        "\n",
        "        log_probs.append(dist.log_prob(action))\n",
        "\n",
        "        speed, reward, done = environment(speed, action.item())\n",
        "        rewards.append(reward)\n",
        "\n",
        "    # -----------------------------\n",
        "    # Policy Gradient Update\n",
        "    # -----------------------------\n",
        "    G = 0\n",
        "    loss = 0\n",
        "\n",
        "    for log_prob, reward in zip(reversed(log_probs), reversed(rewards)):\n",
        "        G = reward + gamma * G\n",
        "        loss -= log_prob * G\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Episode {episode+1}: Total reward = {sum(rewards)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2ftUgjZQybS",
        "outputId": "dd5f1dd6-85f6-4a91-ecea-fff14ab00cfa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1: Total reward = -50\n",
            "Episode 2: Total reward = -9\n",
            "Episode 3: Total reward = -37\n",
            "Episode 4: Total reward = 2\n",
            "Episode 5: Total reward = 2\n",
            "Episode 6: Total reward = 4\n",
            "Episode 7: Total reward = 4\n",
            "Episode 8: Total reward = -3\n",
            "Episode 9: Total reward = 0\n",
            "Episode 10: Total reward = 4\n",
            "Episode 11: Total reward = -3\n",
            "Episode 12: Total reward = 6\n",
            "Episode 13: Total reward = 5\n",
            "Episode 14: Total reward = -3\n",
            "Episode 15: Total reward = 2\n",
            "Episode 16: Total reward = 0\n",
            "Episode 17: Total reward = 1\n",
            "Episode 18: Total reward = 6\n",
            "Episode 19: Total reward = 6\n",
            "Episode 20: Total reward = 6\n",
            "Episode 21: Total reward = 4\n",
            "Episode 22: Total reward = 6\n",
            "Episode 23: Total reward = 6\n",
            "Episode 24: Total reward = 5\n",
            "Episode 25: Total reward = 6\n",
            "Episode 26: Total reward = 6\n",
            "Episode 27: Total reward = 6\n",
            "Episode 28: Total reward = 6\n",
            "Episode 29: Total reward = 6\n",
            "Episode 30: Total reward = 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R7dS0RLIQ5sW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21eZnZYHQjdS",
        "outputId": "09a4d50c-c758-4350-c309-a0c0891cabfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1: Reward=-75 Baseline=-0.87\n",
            "Episode 2: Reward=-150 Baseline=-0.93\n",
            "Episode 3: Reward=-5 Baseline=-0.31\n",
            "Episode 4: Reward=-5 Baseline=-0.31\n",
            "Episode 5: Reward=3 Baseline=0.38\n",
            "Episode 6: Reward=-15 Baseline=-0.58\n",
            "Episode 7: Reward=5 Baseline=0.83\n",
            "Episode 8: Reward=-15 Baseline=-0.58\n",
            "Episode 9: Reward=-6 Baseline=-0.35\n",
            "Episode 10: Reward=5 Baseline=0.83\n",
            "Episode 11: Reward=0 Baseline=0.00\n",
            "Episode 12: Reward=-6 Baseline=-0.35\n",
            "Episode 13: Reward=3 Baseline=0.38\n",
            "Episode 14: Reward=4 Baseline=0.57\n",
            "Episode 15: Reward=3 Baseline=0.38\n",
            "Episode 16: Reward=6 Baseline=1.20\n",
            "Episode 17: Reward=2 Baseline=0.22\n",
            "Episode 18: Reward=6 Baseline=1.20\n",
            "Episode 19: Reward=3 Baseline=0.38\n",
            "Episode 20: Reward=4 Baseline=0.57\n",
            "Episode 21: Reward=6 Baseline=1.20\n",
            "Episode 22: Reward=6 Baseline=1.20\n",
            "Episode 23: Reward=6 Baseline=1.20\n",
            "Episode 24: Reward=1 Baseline=0.10\n",
            "Episode 25: Reward=6 Baseline=1.20\n",
            "Episode 26: Reward=6 Baseline=1.20\n",
            "Episode 27: Reward=6 Baseline=1.20\n",
            "Episode 28: Reward=5 Baseline=0.83\n",
            "Episode 29: Reward=6 Baseline=1.20\n",
            "Episode 30: Reward=2 Baseline=0.22\n"
          ]
        }
      ],
      "source": [
        "# Policy Gradient with Baseline (Variance Reduction)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "\n",
        "# -----------------------------\n",
        "# Policy Network\n",
        "# -----------------------------\n",
        "policy = nn.Sequential(\n",
        "    nn.Linear(1, 16),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(16, 2),\n",
        "    nn.Softmax(dim=1)\n",
        ")\n",
        "\n",
        "optimizer = optim.Adam(policy.parameters(), lr=0.01)\n",
        "gamma = 0.9\n",
        "\n",
        "# -----------------------------\n",
        "# Environment\n",
        "# -----------------------------\n",
        "def environment(speed, action):\n",
        "    speed += 1 if action == 0 else -1\n",
        "    speed = max(0, speed)\n",
        "\n",
        "    if speed == 5:\n",
        "        return speed, 10, True\n",
        "    return speed, -1, False\n",
        "\n",
        "# -----------------------------\n",
        "# Training\n",
        "# -----------------------------\n",
        "for episode in range(30):\n",
        "    speed = 0\n",
        "    log_probs = []\n",
        "    rewards = []\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        state = torch.tensor([[speed]], dtype=torch.float32)\n",
        "\n",
        "        probs = policy(state)\n",
        "        dist = torch.distributions.Categorical(probs)\n",
        "        action = dist.sample()\n",
        "\n",
        "        log_probs.append(dist.log_prob(action))\n",
        "        speed, reward, done = environment(speed, action.item())\n",
        "        rewards.append(reward)\n",
        "\n",
        "    # --------- BASELINE ----------\n",
        "    baseline = sum(rewards) / len(rewards)\n",
        "\n",
        "    G = 0\n",
        "    loss = 0\n",
        "    for log_prob, reward in zip(reversed(log_probs), reversed(rewards)):\n",
        "        G = reward + gamma * G\n",
        "        advantage = G - baseline\n",
        "        loss -= log_prob * advantage\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Episode {episode+1}: Reward={sum(rewards)} Baseline={baseline:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Actor-Critic: Two brains working together\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "\n",
        "# -----------------------------\n",
        "# Actor Network (Policy)\n",
        "# -----------------------------\n",
        "actor = nn.Sequential(\n",
        "    nn.Linear(1, 16),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(16, 2),\n",
        "    nn.Softmax(dim=1)\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Critic Network (Value)\n",
        "# -----------------------------\n",
        "critic = nn.Sequential(\n",
        "    nn.Linear(1, 16),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(16, 1)\n",
        ")\n",
        "\n",
        "actor_opt = optim.Adam(actor.parameters(), lr=0.01)\n",
        "critic_opt = optim.Adam(critic.parameters(), lr=0.01)\n",
        "gamma = 0.9\n",
        "\n",
        "# -----------------------------\n",
        "# Environment\n",
        "# -----------------------------\n",
        "def environment(speed, action):\n",
        "    speed += 1 if action == 0 else -1\n",
        "    speed = max(0, speed)\n",
        "\n",
        "    if speed == 5:\n",
        "        return speed, 10, True\n",
        "    return speed, -1, False\n",
        "\n",
        "# -----------------------------\n",
        "# Training\n",
        "# -----------------------------\n",
        "for episode in range(30):\n",
        "    speed = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        state = torch.tensor([[speed]], dtype=torch.float32)\n",
        "\n",
        "        probs = actor(state)\n",
        "        dist = torch.distributions.Categorical(probs)\n",
        "        action = dist.sample()\n",
        "\n",
        "        value = critic(state)\n",
        "\n",
        "        next_speed, reward, done = environment(speed, action.item())\n",
        "        next_state = torch.tensor([[next_speed]], dtype=torch.float32)\n",
        "\n",
        "        next_value = critic(next_state)\n",
        "\n",
        "        # Advantage\n",
        "        advantage = reward + gamma * next_value - value\n",
        "\n",
        "        # Actor loss\n",
        "        actor_loss = -dist.log_prob(action) * advantage.detach()\n",
        "\n",
        "        # Critic loss\n",
        "        critic_loss = advantage.pow(2)\n",
        "\n",
        "        actor_opt.zero_grad()\n",
        "        critic_opt.zero_grad()\n",
        "\n",
        "        actor_loss.backward()\n",
        "        critic_loss.backward()\n",
        "\n",
        "        actor_opt.step()\n",
        "        critic_opt.step()\n",
        "\n",
        "        speed = next_speed\n",
        "\n",
        "    print(f\"Episode {episode+1}: Finished at speed {speed}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQS4HaKbRmBd",
        "outputId": "16e71e42-6dd2-408d-eefb-3fa3972fe01b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1: Finished at speed 5\n",
            "Episode 2: Finished at speed 5\n",
            "Episode 3: Finished at speed 5\n",
            "Episode 4: Finished at speed 5\n",
            "Episode 5: Finished at speed 5\n",
            "Episode 6: Finished at speed 5\n",
            "Episode 7: Finished at speed 5\n",
            "Episode 8: Finished at speed 5\n",
            "Episode 9: Finished at speed 5\n",
            "Episode 10: Finished at speed 5\n",
            "Episode 11: Finished at speed 5\n",
            "Episode 12: Finished at speed 5\n",
            "Episode 13: Finished at speed 5\n",
            "Episode 14: Finished at speed 5\n",
            "Episode 15: Finished at speed 5\n",
            "Episode 16: Finished at speed 5\n",
            "Episode 17: Finished at speed 5\n",
            "Episode 18: Finished at speed 5\n",
            "Episode 19: Finished at speed 5\n",
            "Episode 20: Finished at speed 5\n",
            "Episode 21: Finished at speed 5\n",
            "Episode 22: Finished at speed 5\n",
            "Episode 23: Finished at speed 5\n",
            "Episode 24: Finished at speed 5\n",
            "Episode 25: Finished at speed 5\n",
            "Episode 26: Finished at speed 5\n",
            "Episode 27: Finished at speed 5\n",
            "Episode 28: Finished at speed 5\n",
            "Episode 29: Finished at speed 5\n",
            "Episode 30: Finished at speed 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-xhoB1rwUevP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}